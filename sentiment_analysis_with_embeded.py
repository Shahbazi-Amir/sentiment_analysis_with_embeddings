# install neccery libs
!pip install pandas numpy scikit-learn nltk gensim
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from gensim.models import KeyedVectors
nltk.download('stopwords')
from nltk.corpus import stopwords

# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ stopword
print(stopwords.words('english')[:5])
# Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨
file_path = 'training.1600000.processed.noemoticon.csv'
column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']

df = pd.read_csv(
    file_path,
    header=0,  # Ø³Ø·Ø± Ø§ÙˆÙ„ header Ù‡Ø³Øª
    encoding='latin1',
    names=column_names,
    low_memory=False
)

# ÙÛŒÙ„ØªØ± ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ 0 Ùˆ 4
df = df[df['sentiment'].isin([0, 4])]

print("âœ… ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:", len(df))
print("ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§:")
print(df['sentiment'].value_counts())
from nltk.corpus import stopwords
import re

# ØªÙ†Ø¸ÛŒÙ… stop words + Ú©Ù„Ù…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
stop_words = set(stopwords.words('english'))
extra_stopwords = {'rt', 'user', 'url', 'â€¦', 'amp', 'via', 'http', 'https'}
all_stopwords = stop_words.union(extra_stopwords)

# ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ù‡ØªØ± Ø´Ø¯Ù‡
def preprocess_text_improved(text):
    text = text.lower()
    text = re.sub(r'@\S+', '', text)
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'#\S+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = text.split()
    filtered_tokens = [word for word in tokens if word not in all_stopwords]
    return filtered_tokens

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
df['tokens'] = df['text'].apply(preprocess_text_improved)
import gensim.downloader as api

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ "glove-twitter-25" - Ø­Ø¯ÙˆØ¯ 86 Ù…Ú¯Ø§Ø¨Ø§ÛŒØª
embedding_model = api.load("glove-twitter-25")

print("âœ… Ù…Ø¯Ù„ embedding Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!")


import numpy as np

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ tokens Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GloVe)
def text_to_vector(tokens, model, vector_size=25):
    vectors = [model[word] for word in tokens if word in model]
    if not vectors:
        return np.zeros(vector_size)
    return np.mean(vectors, axis=0)

# Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X = np.array(df['tokens'].apply(text_to_vector, model=embedding_model).tolist())
y = df['sentiment'].map({0: 0, 4: 1})  # 0 = Ù…Ù†ÙÛŒØŒ 1 = Ù…Ø«Ø¨Øª

print("âœ… Ø§Ø¨Ø¹Ø§Ø¯ X:", X.shape)
print("âœ… Ø§Ø¨Ø¹Ø§Ø¯ y:", y.shape)



from sklearn.model_selection import train_test_split

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ train Ùˆ test
X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.2,     # 80% train, 20% test
    random_state=42,   # Ø«Ø§Ø¨Øª Ú©Ø±Ø¯Ù† Ø±Ù†Ø¯ÙˆÙ… Ø¨Ø±Ø§ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÚ©Ø±Ø§Ø±
    stratify=y         # Ø­ÙØ¸ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± train Ùˆ test
)

# Ú†Ø§Ù¾ Ø§Ø¨Ø¹Ø§Ø¯
print("âœ… X_train shape:", X_train.shape)
print("âœ… X_test shape:", X_test.shape)
print("âœ… y_train shape:", y_train.shape)
print("âœ… y_test shape:", y_test.shape)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ test
y_pred = model.predict(X_test)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ùˆ Ú¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
accuracy = accuracy_score(y_test, y_pred)
print(f"ğŸ¯ Ø¯Ù‚Øª Ù…Ø¯Ù„: {accuracy:.4f}")
print("\nğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print(classification_report(y_test, y_pred))


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø§Ø´ØªØ¨Ø§Ù‡
cm = confusion_matrix(y_test, y_pred)

# Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø§Ø´ØªØ¨Ø§Ù‡
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

from sklearn.model_selection import cross_val_score
import numpy as np

# Ø§Ø¬Ø±Ø§ÛŒ 5-Fold Cross Validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬
print("ğŸ¯ Ø¯Ù‚Øª Ø¯Ø± Ù‡Ø± Fold:")
print(cv_scores)
print(f"ğŸ“Œ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ù‚Øª: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")